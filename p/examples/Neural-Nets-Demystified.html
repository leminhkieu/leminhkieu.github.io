<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>Neural Nets Demystified</title>
    <meta name="description" content="How to use neural nets to solve real world problems, without a lot of jargon... or maybe just a little. Presented at the Portland Data Science meetup at Little Bird.">
    <meta name="author" content="Hobson Lane" />
    <meta name="dcterms.date" content="2015-05-19" />

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
              <section>
      <h2 id="neural-nets-demystified">Neural Nets Demystified</h2>
      <ol>
        <li>Demystify</li>
        <li>Dig Deeper</li>
      </ol>
      <aside class="notes">
      <p>First I’ll show you a simple example (predicting Portland Weather).</p>
      <p>Then I’ll show you how to play around at the frontier of the state of the art</p>
<ul>
  <li>Thoughts about the upcoming <a href="http://www.meetup.com/Portland-Data-Science-Group/events/222322211/">PDX Data Science Meetup</a></li>
  <li><a href="/Data-Science-Meetup--Neural-Nets-Demystified/">“Neural Nets Demystified.”</a></li>
</ul>

</aside>
</section>
<section>


<h2 id="classification">Classification</h2>

<p>The most basic ML task is classification</p>

<p>In NN lingo, this is called “association”</p>

<p>So lets predict “rain” (1) “no rain” (0) for PDX tomorrow</p>

</section><section>

<h2 id="supervised-learning">Supervised Learning</h2>

<p>We have historical “examples” of rain and shine</p>

<p><a href="http://wunderground.org">Weather Underground</a></p>

<p>Since we know the classification (training set)…</p>

<p>Supervised classification (association)</p>

</section><section>

<h2 id="rain-shine-partly-cloudy-">Rain, Shine, Partly-Cloudy ?</h2>

<p>Wunderground lists several possible “conditions” or classes</p>

<p>If we wanted to predict them all </p>

<p>We would just make a binary classifier for each one</p>

<p>All classification problems can be reduced a binary classification</p>

</section><section>

<h2 id="perceptronhttpsenwikipediaorgwikiperceptron"><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Perceptron</em></a></h2>

<p>Sounds mysterious, like a “flux capacitor” or something…</p>

<p>It’s just a multiply and threshold check:</p>

<pre><code class="hightlight" data-lang="python">if (weights * inputs) > 0:
    output = 1
else:
    output = 0
</code></pre>
</section><section>
<h2 id="need-something-better">Need something better</h2>
<ul>
<li>Fine for “using” (<em><a href="https://en.wikipedia.org/wiki/Activation_function">activating</a></em>) a NN</li>
<li>But for "training" (<em><a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a></em>) need ...</li>
  <ul>
    <li>Doesn’t change direction: <em><a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonic</a></em></li>
    <li>Doesn’t jump around: <em><a href="https://en.wikipedia.org/wiki/Smoothness">smooth</a></em></li>
  </ul>
</ul>

</section><section>

<h2 id="sigmoid"><a href="https://en.wikipedia.org/wiki/Perceptron"><em>Sigmoid</em></a></h2>

<p>Again, sounds mysterious… like a transcendental function</p>

<p>It is a transcendental function, but the word just means</p>

<p>Curved, smooth like the letter “C”</p>

</section><section>

<h2 id="what-greek-letter-do-you-think-of-when-i-say-sigma">What Greek letter do you think of when I say “Sigma”?</h2>

<h3 id="section">“Σ”</h3>

<p>What Roman (English) character?</p>

<ul>
  <li>“E”?</li>
  <li>“S”?</li>
  <li>“C”?</li>
</ul>

</section><section>

<h2 id="sigmahttpsenwikipediaorgwikisigma"><a href="https://en.wikipedia.org/wiki/Sigma">Sigma</a></h2>

<p>You didn’t know this was a Latin class, did you…</p>

<ul>
<li> Σ (uppercase)</li>
<li> σ (lowercase)</li>
<li> ς (last letter in word)</li>
<li> c (alternatively)</li>
</ul>

</section><section>

<p>Most English speakers think of an “S” when they hear “Sigma”.</p>
<p>So the meaning has evolved to mean S-shaped.</p>

<H2>That’s what we want</H2>
<p> something smooth, shaped like an “S”</p>
<p> so it goes from 0 to 1 in an S shape</p>

</section><section class="right-align-all lightest-body-text" data-background="images/neural-nets-demystified.png">

  <h2 id="perceptron">Perceptron</h2><br><br><br><br><br><br><br><br><br><br><br><br>

</section><section class="right-align-all lightest-body-text" data-background="images/regression.png">

  <h2 id="regression">Linear</h2><br><br><br><br><br><br><br><br><br><br><br><br>

</section><section class="right-align-all lightest-body-text" data-background="images/time-series.png">

  <h2 id="regression">Time Series</h2><br><br><br><br><br><br><br><br><br><br><br><br>

</section><section>

  <h2>The weights in a NN form a sequence of matrices</h2>
  <p>One matrix for each mess of connections between layers</p>
  <p>Once you've trained the NN you can disply them as heat maps</p>
  <p>Look for structure and oportunities to "prune"</p>

</section><section>

  <h2>Input biases are the 1st column of weights</h2>
  <p> (in the first matrix of weights) </p>
  <div id="impress" class="impress-not-supported">
    <div style="height: 100%;">
      <p><img src="images/W_output_column.png" alt="weight output column vector" /></p>
    </div>
  </div>
</section><section>

  <h2>Output biases are the first row of weights</h2>
  <p> (in the last matrix of weights) </p>
  <div id="impress" class="impress-not-supported">
    <div style="height: 100%;">
      <p><img src="images/W_input_row.png" alt="weight input row vector" /></p>
    </div>
  </div>

</section>
<section class="right-align-all lightest-body-text" data-background="images/W_h.png">
<div class="transp-background" style="width: 80%; float: right">
  <br>
  <h2>6 Perceptrons = 6 Rows</h2>
  <h2>13 Outputs = 13 columns</h2>
  <br>
</div>

</section><section>

<H2 id="trainer">Trainer (<em>(backpropagator)[https://en.wikipedia.org/wiki/Backpropagation]</em>)</H2>

<p>can predict the change in <code>weights</code> required
Wants to nudge the <code>output</code> closer to the <code>target</code></p>

<p><code>target</code>: known classification for training examples
<code>output</code>: predicted classification your network spits out</p>

</section><section>

<h2 id="but-just-a-nudge">But just a nudge.</h2>

<p>Don’t get greedy and push all the way to the answer
Because your linear sloper predictions are wrong
And there may be nonlinear interactions between the weights (multiply layers)</p>

<p>So set the learning rate (\alpha) to somthething less than 1
the portion of the predicted nudge you want to “dial back” to</p>

</section><section>

<h2 id="example-predict-rain-in-portland">Example: Predict Rain in Portland</h2>

<ul>
  <li>PyBrain</li>
  <li>pug-ann (helper functions TBD PyBrain2)</li>
</ul>

</section><section>

<p>Get historical weather for Portland then …</p>

<ol>
  <li>Backpropagate: train a perceptron</li>
  <li>Activate: predict the weather for tomorrow!</li>
</ol>

</section><section>

<p>NN Advantages</p>

<ul>
  <li>Easy
    <ul>
      <li>No math!</li>
      <li>No tuning!</li>
      <li>Just plug and chug.</li>
    </ul>
  </li>
  <li>General
    <ul>
      <li>One model can apply to many problems</li>
    </ul>
  </li>
  <li>Advanced
    <ul>
      <li>They often beat all other “tuned” approaches</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Disadvantage #1: Slow training</p>

<ul>
  <li>24+ hr for complex Kaggle example on laptop</li>
  <li>90x30x20x10 model degrees freedom
    <ul>
      <li>90 input dimensions (regressors)</li>
      <li>30 nodes for <em>hidden layer</em> 1</li>
      <li>20 nodes for <em>hidden layer</em> 2</li>
      <li>10 output dimensions (predicted values)</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Disadvantage #2: They don’t scale (unparallelizable)</p>

<ul>
  <li>Fully-connected NNs can’t be <em>easily</em> hyper-parallelized (GPU)
    <ul>
      <li>Large matrix multiplications</li>
      <li>Layers depend on all elements of previous layers</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Scaling Workaround</p>

<p>At Kaggle workshop we discussed paralleling linear algebra</p>

<ul>
  <li>Split matrices up and work on “tiles”</li>
  <li>Theano, <a href="">Keras</a> for python</li>
  <li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">PLASMA</a> for BLAS</li>
</ul>

</section><section>

<p>Scaling Workaround Limitations</p>

<p>But tiles must be shared/consolidated and theirs redundancy</p>

<ul>
  <li>Data flow: Main -&gt; CPU -&gt; GPU -&gt; GPU cache (and back)</li>
  <li>Data com (RAM xfer) is limiting</li>
  <li>Data RAM size (at each stage) is limiting </li>
  <li><a href="http://icl.cs.utk.edu/news_pub/submissions/plasma-scidac09.pdf">Each GPU is equivalent to 16 core node</a></li>
</ul>

</section><section>

<p>Disadvantage #3: They overfit</p>

<ul>
  <li>Too manu nodes = overfitting</li>
</ul>

</section><section>

<p>What is the big O?</p>

<ul>
  <li>Degrees of freedom grow with number of nodes &amp; layers</li>
  <li>Each layer’s nodes connected to each previous layer’s</li>
  <li>That a lot of wasted “freedom”</li>
</ul>

<h2 id="on2">O(N^2)</h2>

</section><section>

<p>Rule of thumb </p>

<p>NOT <code>N**2</code> </p>

<p>But <code>M * N**2</code></p>

<p>N: number of nodes
M: number of layers</p>

</section><section>

<p><code>assert(M * N**2 &lt; len(training_set) / 10.)</code></p>

<p>I’m serious… put this into your code.
I wasted a lot of time training models for Kaggle that overfitted.</p>

</section><section>

<p>You do need to know math!</p>

<ul>
  <li>To imprint your net with the structure (math) of the problem
    <ul>
      <li>Feature analysis or transformation (conventional ML)</li>
      <li>Choosing the activation function and segmenting your NN</li>
    </ul>
  </li>
  <li>Prune and evolve your NN</li>
</ul>

</section><section>

<p>This is a virtuous cycle!</p>

<ul>
  <li>More structure (no longer fully connected)
    <ul>
      <li>Each independent path (segment) is parallelizable!</li>
    </ul>
  </li>
  <li>Automatic tuning, pruning, evolving is all parallelizable!
    <ul>
      <li>Just train each NN separately</li>
      <li>Check back in with Prefrontal to “compete”</li>
    </ul>
  </li>
</ul>

</section><section>

<p>Structure you can play with (textbook)</p>

<ul>
  <li>limit connections </li>
</ul>

<p>jargon: <em>receptive fields</em></p>

<ul>
  <li>limit weights </li>
</ul>

<p>jargon: <em>weight sharing</em></p>

<p>All the rage: <em>convolutional networks</em></p>

</section><section>

<p>Unconventional structure to play with</p>

<p>New ideas, no jargon yet, just crackpot names</p>

<ul>
  <li>limit weight ranges (e.g. -1 to 1, 0 to 1, etc)</li>
  <li>weight “snap to grid” (snap learning)</li>
</ul>

</section><section>

<p>Joke: “What’s the difference between a scientist and a crackpot?”</p>

</section><section>

<p>Ans: “P-value”</p>

<ul>
  <li>No <string>P</strong>HD</li>
  <li>High-<strong>P</strong>robability null hypothesis</li>
  <li>Not <strong>P</strong>ublished</li>
  <li>Not <strong>P</strong>eer-reviewed</li>
  <li>No <strong>P</strong>yPi <strong>p</strong>ackage</li>
</ul>

<p>I’m a crackpot!</p>

</section><section>

<p>Resources</p>

<ul>
  <li><a href="http://keras.io/">keras.io</a>: Scalable Python NNs</li>
  <li><a href="http://hagan.okstate.edu/NNDesign.pdf">Neural Network Design</a>: Free NN Textbook!</li>
  <li><a href="https://github.com/hobson/pug-ann">pug-ann</a>: Helpers for PyBrain and Wunderground</li>
  <li><a href="https://github.com/pybrain2/pybrain2">PyBrain2</a>: We’re working on it</li>
</ul>

</section>

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
